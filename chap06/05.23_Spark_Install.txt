Spark 설치방법
설치 사이트 https://spark.apache.org/downloads.html 들어가서 버전 확인.
Download Spark: spark-3.1.3-bin-hadoop3.2.tgz 클릭. 접속 후 링크 복사.

https://dlcdn.apache.org/spark/spark-3.1.3/spark-3.1.3-bin-hadoop3.2.tgz

1. wget https://dlcdn.apache.org/spark/spark-3.1.3/spark-3.1.3-bin-hadoop3.2.tgz (입력)
2. tar -xvzf spark-3.1.3-bin-hadoop3.2.tgz (압축풀기)
3. mv spark-3.1.3-bin-hadoop3.2 spark-3.1.3 (폴더명 변경)
4. nano .bashrc (열기)
5. (맨 밑에 환경설정 등록)
export SPARK_HOME=/home/hduser/spark-3.1.3
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export SPARK_CONF_DIR=$SPARK_HOME/conf
export SPARK_MASTER_HOST=localhost
export PYSPARK_PYTHON=python3
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9-src.zip:$PYTHONPATH
export PATH=$SPARK_HOME/bin:$SPARK_HOME/python:$PATH
6. source .bashrc (한번 실행해주기)
7. echo $SPARK_HOME (경로 확인)
8. spark-shell (실행하는지 확인 spark-shell실행(windows의 cmd))
# 실행이안된다면 아래 해결법

해결법
1. spark-3.1.3/bin으로 이동.
2. spark-shell 실행
3. 안된다면 삭제 후 재실행

# spark, python, java, R 지원

9. scala 언어를 사용하여 spark를 사용해야함.
10. :quit or ctrl+D (spark종료)
# python 언어를 사용하려면 
1. pyspark (pyspark 실행)

# python에서 pyspark를 찾을 수 있는 module설치
1. pip3 install findspark
